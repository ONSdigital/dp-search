{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Regression Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Request(abc.ABC):\n",
    "    def __init__(self, host, uri):\n",
    "        self._host = host\n",
    "        self._uri = uri\n",
    "        \n",
    "    def target(self, query, page):\n",
    "        return self._host + self._uri + \"?q=\" + query + \"&page=%d\" % page\n",
    "        \n",
    "    def search(self, query, page=1):\n",
    "        import requests\n",
    "        \n",
    "        r = requests.get(self.target(query, page))\n",
    "        return r.json()\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_hits(self, query):\n",
    "        pass\n",
    "    \n",
    "class BabbageRequest(Request):\n",
    "    host = \"http://localhost:20000/\"\n",
    "    uri = \"search/data\"\n",
    "    def __init__(self):\n",
    "        super(BabbageRequest, self).__init__(self.host, self.uri)\n",
    "        \n",
    "    def target(self, query, page):\n",
    "        return self._host + self._uri + \"?q=\" + query + \"&page=%d\" % page + \"&searchTarget=internal\"\n",
    "        \n",
    "    def get_hits(self, query, page):\n",
    "        response = self.search(query, page=page)\n",
    "        hits = []\n",
    "        for hit in response['result']['results']:\n",
    "            hits.append(hit.get('uri'))\n",
    "            \n",
    "        return hits\n",
    "        \n",
    "class SearchRequest(Request):\n",
    "    host = \"http://localhost:5000/\"\n",
    "    uri = \"search/ons/content\"\n",
    "    def __init__(self):\n",
    "        super(SearchRequest, self).__init__(self.host, self.uri)\n",
    "        \n",
    "    def get_hits(self, query, page):\n",
    "        response = self.search(query, page=page)\n",
    "        hits = []\n",
    "        for hit in response['results']:\n",
    "            hits.append(hit.get('uri'))\n",
    "            \n",
    "        return hits\n",
    "    \n",
    "class ConceptualSearchRequest(SearchRequest):\n",
    "    uri = \"search/conceptual/ons/content\"\n",
    "\n",
    "br = BabbageRequest()\n",
    "sr = SearchRequest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "\n",
    "class SearchStats(object):\n",
    "    db = \"local\"\n",
    "    collection = \"searchstats\"\n",
    "    def __init__(self):\n",
    "        self._client = pymongo.MongoClient()\n",
    "        self._db = self._client.get_database(self.db)\n",
    "        self._collection = self._db.get_collection(self.collection)\n",
    "        \n",
    "        self._docs = []\n",
    "        \n",
    "        self._load()\n",
    "        \n",
    "    def _load(self):\n",
    "        if len(self._docs) == 0:\n",
    "            for doc in self._collection.find():\n",
    "                self._docs.append(doc)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self._docs)\n",
    "                \n",
    "    def __iter__(self):\n",
    "        for doc in self._docs:\n",
    "            yield doc\n",
    "            \n",
    "    def __getitem__(self, item):\n",
    "        return self._docs[item]\n",
    "    \n",
    "    def group_by_search_term(self):\n",
    "        grouped = {}\n",
    "        \n",
    "        for doc in self._docs:\n",
    "            term = doc.get(\"term\")\n",
    "            \n",
    "            if term not in grouped:\n",
    "                grouped[term] = []\n",
    "            grouped[term].append(doc)\n",
    "            \n",
    "        return grouped\n",
    "    \n",
    "    def judgements(self, max_judgement=4.):\n",
    "        \"\"\"\n",
    "        Groups searchStats by search term\n",
    "        \"\"\"\n",
    "        judgements = {}\n",
    "        for doc in self._docs:\n",
    "            rank = doc.get(\"linkindex\") + ((doc.get(\"pageindex\") - 1) * doc.get(\"pagesize\"))\n",
    "            term = doc.get(\"term\")\n",
    "            if term not in judgements:\n",
    "                judgements[term] = {}\n",
    "                \n",
    "            url = doc.get('url')\n",
    "            if url not in judgements[term]:\n",
    "                judgements[term][url] = {\"count\": 1, \"rank\": rank}\n",
    "            else:\n",
    "                judgements[term][url][\"count\"] += 1\n",
    "                \n",
    "        # Normalise\n",
    "        for key in judgements:\n",
    "            # Sort\n",
    "            sorted_values = sorted(judgements[key].items(), key=lambda kv: kv[1][\"count\"])\n",
    "            \n",
    "            # Normalise\n",
    "            j = np.linspace(0, max_judgement, len(sorted_values))\n",
    "            for i, item in enumerate(sorted_values):\n",
    "                k, v = item\n",
    "                judgements[key][k][\"judgement\"] = j[i]\n",
    "            \n",
    "            \n",
    "        return judgements\n",
    "    \n",
    "search_stats = SearchStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SCORE = 4.0\n",
    "\n",
    "def idealJudgement(num):\n",
    "    i = 0\n",
    "    incremenet = (1.0 / (float(num) - 1.0)) * num\n",
    "    \n",
    "    iJ = np.zeros(num)\n",
    "    val = len(iJ)\n",
    "    while (val > 0):\n",
    "        iJ[i] = (val / float(num)) * MAX_SCORE\n",
    "        i += 1\n",
    "        val -= incremenet\n",
    "        \n",
    "    return iJ\n",
    "\n",
    "def idealDiscountedCumulativeGain(num):\n",
    "    idealGain = idealJudgement(num)\n",
    "    iDCG = np.zeros(num)\n",
    "    \n",
    "    total = 0.0\n",
    "    for i in range(num):\n",
    "        total += idealGain[i] / float(i+1)\n",
    "        iDCG[i] = total\n",
    "    return iDCG\n",
    "\n",
    "class NDCG(object):\n",
    "    def __init__(self, judgements):\n",
    "        self.judgements = judgements\n",
    "        \n",
    "    def dcg(self):        \n",
    "        \n",
    "        dcg_dict = {}\n",
    "        for key in self.judgements:\n",
    "            dcg_dict[key] = {\"dcg\": [], \"urls\": []}\n",
    "            \n",
    "        for key in dcg_dict:\n",
    "            total = 0.0\n",
    "            judgements = self.judgements[key]\n",
    "            for url in judgements:\n",
    "                judgement = judgements[url]\n",
    "                total += judgement[\"judgement\"] / float(judgement[\"rank\"])\n",
    "                dcg_dict[key][\"dcg\"].append(total)\n",
    "                dcg_dict[key][\"urls\"].append(url)\n",
    "            \n",
    "        return dcg_dict\n",
    "    \n",
    "    def ndcg(self):\n",
    "        \n",
    "        dcg_dict = self.dcg()\n",
    "        ndcg_dict = {}\n",
    "        \n",
    "        for key in dcg_dict.keys():\n",
    "            dcg_data = dcg_dict[key]\n",
    "            dcg = dcg_data[\"dcg\"]\n",
    "            \n",
    "            idcg = idealDiscountedCumulativeGain(len(dcg))\n",
    "\n",
    "            ndcg = np.zeros(len(dcg))\n",
    "\n",
    "            for i in range(len(ndcg)):\n",
    "                ndcg[i] = min(1.0, dcg[i] / idcg[i])\n",
    "                \n",
    "            ndcg_dict[key] = {}\n",
    "            ndcg_dict[key][\"ndcg\"] = ndcg\n",
    "            ndcg_dict[key][\"urls\"] = dcg_data[\"urls\"]\n",
    "        return ndcg_dict\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.judgements.__iter__()\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.judgements[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.judgements)\n",
    "        \n",
    "ndcg = NDCG(search_stats.judgements())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_by_terms = search_stats.group_by_search_term()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTest(object):\n",
    "    \"\"\"\n",
    "    Tests that babbage and external search (not conceptual API) give the same results\n",
    "    \"\"\"\n",
    "    def __init__(self, search_terms):\n",
    "        self.search_terms = search_terms\n",
    "        \n",
    "    def run(self):\n",
    "        from json import JSONDecodeError\n",
    "        \n",
    "        br = BabbageRequest()\n",
    "        sr = SearchRequest()\n",
    "        \n",
    "        for search_term in self.search_terms:\n",
    "            page = 1\n",
    "            while True:\n",
    "                try:\n",
    "                    babbage_results = br.get_hits(search_term, page)\n",
    "                    search_results = sr.get_hits(search_term, page)\n",
    "\n",
    "                    assert babbage_results == search_results, \"Regression test failed for query '%s' on page %d\" % (search_term, page)\n",
    "                    page += 1\n",
    "                except JSONDecodeError:\n",
    "                    # No more pages available\n",
    "                    print(search_term + \":\", \"PASS (total pages=%d)\" % page)\n",
    "                    break\n",
    "        print(\"TEST COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpi: PASS (total pages=42)\n",
      "gender pay gap: PASS (total pages=25)\n",
      "cpi: PASS (total pages=162)\n",
      "gdp: PASS (total pages=53)\n",
      "inflation: PASS (total pages=183)\n",
      "crime: PASS (total pages=14)\n",
      "unemployment: PASS (total pages=53)\n",
      "population: PASS (total pages=83)\n",
      "immigration: PASS (total pages=20)\n",
      "mental health: PASS (total pages=64)\n",
      "london: PASS (total pages=47)\n",
      "london population: PASS (total pages=120)\n",
      "retail price index: PASS (total pages=482)\n",
      "life expectancy: PASS (total pages=38)\n",
      "obesity: PASS (total pages=2)\n",
      "religion: PASS (total pages=8)\n",
      "migration: PASS (total pages=24)\n",
      "poverty: PASS (total pages=3)\n",
      "social media: PASS (total pages=97)\n",
      "employment: PASS (total pages=138)\n",
      "TEST COMPLETE\n"
     ]
    }
   ],
   "source": [
    "search_terms = search_stats.group_by_search_term().keys()\n",
    "\n",
    "regression_tester = RegressionTest(search_terms)\n",
    "regression_tester.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
