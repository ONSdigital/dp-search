{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning of Keywords using fastText\n",
    "## Text Classification\n",
    "The goal of text classification is to assign documents (such as emails, posts, text messages, product reviews, etc...) to one or multiple categories. Such categories can be review scores, spam v.s. non-spam, or the language in which the document was typed. Nowadays, the dominant approach to build such classifiers is machine learning, that is learning classification rules from examples. In order to build such classifiers, we need labeled data, which consists of documents and their corresponding categories (or tags, or labels).\n",
    "\n",
    "As an example, we build a classifier which automatically classifies ONS publications by their supplied keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fastText\n",
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.isdir(\"./models/\") is False:\n",
    "    !mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpa\n",
    "To create the text corpus, we load in articles and bulletins published on the ONS website. The final corpus consists of sentences found in the pages summaries and markdown sections, tagged by the keywords provided with each page.\n",
    "\n",
    "For the purpose of this notebook, we have stored all artices and bulletins in a local mongoDB database for painless retrieval. Lets define some functions for loading the pages as json below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMongoDBClient(mongoUrl, port):\n",
    "    from pymongo import MongoClient\n",
    "    return MongoClient(mongoUrl, port)\n",
    "\n",
    "def load_pages(use_mongo=True):\n",
    "    # Load pages from disk/mongo\n",
    "    pages = None\n",
    "    print \"Loading pages from \" + \"mongoDB\" if use_mongo else \"filesystem\"\n",
    "    if (use_mongo):\n",
    "        mongoClient = getMongoDBClient(\"localhost\", 27017)\n",
    "        collection = mongoClient.local.pages\n",
    "\n",
    "        query = {\n",
    "            \"sections\": {\n",
    "                \"$exists\": True\n",
    "            }\n",
    "        }\n",
    "        cursor = collection.find(query)\n",
    "\n",
    "        pages = []\n",
    "        for doc in cursor:\n",
    "            pages.append(doc)\n",
    "    else:\n",
    "        # Read from filesystem\n",
    "        from modules.ONS.file_scanner import FileScanner\n",
    "        scanner = FileScanner()\n",
    "        pages = scanner.load_pages()\n",
    "    print \"Done\"\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pages from mongoDB\n",
      "Done\n",
      "Loaded 1960 pages\n"
     ]
    }
   ],
   "source": [
    "pages = load_pages(use_mongo=True)\n",
    "print \"Loaded %d pages\" % (len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test processing\n",
    "To give our model the best chance of accurate classifications, we should clean up the raw text keywords to remove things like stop words etc. Below we define some basic utility functions to clean raw text, then process the pages we just loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_stopwords():\n",
    "    return set(stopwords.words('english'))  # nltk stopwords list\n",
    "\n",
    "def get_bigram(train_texts):\n",
    "    import gensim\n",
    "    bigram = gensim.models.Phrases(train_texts)  # for bigram collocation detection\n",
    "    return bigram\n",
    "\n",
    "def build_texts_from_file(fname):\n",
    "    import gensim\n",
    "    \"\"\"\n",
    "    Function to build tokenized texts from file\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    fname: File to be read\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    yields preprocessed line\n",
    "    \"\"\"\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield gensim.utils.simple_preprocess(line, deacc=True, min_len=3)\n",
    "\n",
    "def build_texts_as_list_from_file(fname):\n",
    "    return list(build_texts_from_file(fname))\n",
    "\n",
    "def build_texts(texts):\n",
    "    import gensim\n",
    "    \"\"\"\n",
    "    Function to build tokenized texts from file\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    fname: File to be read\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    yields preprocessed line\n",
    "    \"\"\"\n",
    "    for line in texts:\n",
    "        yield gensim.utils.simple_preprocess(line, deacc=True, min_len=3)\n",
    "\n",
    "def build_texts_as_list(texts):\n",
    "    return list(build_texts(texts))\n",
    "\n",
    "def process_texts(texts, stops=get_stopwords()):\n",
    "    \"\"\"\n",
    "    Function to process texts. Following are the steps we take:\n",
    "    \n",
    "    1. Stopword Removal.\n",
    "    2. Collocation detection.\n",
    "    3. Lemmatization (not stem since stemming can reduce the interpretability).\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    texts: Tokenized texts.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    texts: Pre-processed tokenized texts.\n",
    "    \"\"\"\n",
    "    bigram = get_bigram(texts)\n",
    "\n",
    "    texts = [[word for word in line if word not in stops] for line in texts]\n",
    "    texts = [bigram[line] for line in texts]\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    texts = [[word for word in lemmatizer.lemmatize(' '.join(line), pos='v').split()] for line in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1787\n"
     ]
    }
   ],
   "source": [
    "import markdown, re\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "delimiter = \",\"\n",
    "pattern = \"[, \\-!?:]+\"\n",
    "fix_encoding = lambda s: s.decode('utf8', 'ignore')\n",
    "\n",
    "def markdown_to_text(md):\n",
    "    extensions = ['extra', 'smarty']\n",
    "    html = markdown.markdown(md, extensions=extensions, output_format='html5')\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return soup.text\n",
    "\n",
    "def process(text):\n",
    "    content = []\n",
    "    if len(text) > 0:\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (len(filter(None, re.split(pattern, sentence))) > 10):\n",
    "                content.append(fix_encoding(sentence.encode(\"utf-8\").strip()))\n",
    "    return content\n",
    "\n",
    "def parse_page(page):\n",
    "    description = page[\"description\"]\n",
    "    keywords = description[\"keywords\"]\n",
    "                   \n",
    "    sentences = []\n",
    "    if \"summary\" in description:\n",
    "        sentences.extend(process(description[\"summary\"]))\n",
    "    if \"sections\" in page:\n",
    "        for section in page[\"sections\"]:\n",
    "            if \"markdown\" in section:\n",
    "                markdown = section[\"markdown\"]\n",
    "                text = markdown_to_text(markdown)\n",
    "                sentences.extend(process(text))\n",
    "                   \n",
    "    # Collect list of unique, clean keywords\n",
    "    labels = []\n",
    "    for entry in keywords:\n",
    "        entry = entry.strip().lower()\n",
    "        # Replace spaces in keywords with '_', so the classifier identifies them as a single 'word'\n",
    "        entry = re.sub( '\\s+', '_', entry )\n",
    "        labels.append(entry)\n",
    "    \n",
    "    if len(labels) > 0 and len(sentences) > 0:\n",
    "        return {\"sentences\": sentences, \"labels\": labels}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "data = []\n",
    "for page in pages:\n",
    "    if \"description\" in page and \"keywords\" in page[\"description\"] and len(page[\"description\"][\"keywords\"]) > 0:\n",
    "        d = parse_page(page)\n",
    "        if d is not None:\n",
    "            data.append(d)\n",
    "\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the training set\n",
    "The final step is to write the training data out in the correct format. We prepend a prefix to each label, so the classifier knows how to identify them aside from the raw text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = get_stopwords()\n",
    "\n",
    "def label_sentences(sentences, labels, label_prefix=\"__label__\"):\n",
    "    # Clean up the labels by removing stop words etc.\n",
    "    labels = build_texts_as_list(labels)\n",
    "    # Filter out empty keywords\n",
    "    labels = filter(None, process_texts(labels, stops=stops))\n",
    "    \n",
    "    sentences = build_texts_as_list(sentences)\n",
    "    sentences = filter(None, process_texts(sentences, stops=stops))\n",
    "    \n",
    "    labels = set( [\"%s%s\" % (label_prefix, l[0]) for l in labels] )\n",
    "    joined_labels = \" \".join(labels)\n",
    "    \n",
    "    labelled_sentences = []\n",
    "    for sentence in sentences:\n",
    "        text = \" \".join(sentence)\n",
    "#         labelled_sentences.append( \"%s %s\" % (joined_labels, text) )\n",
    "        labelled_sentences.append( (joined_labels, text) )\n",
    "\n",
    "    return labelled_sentences\n",
    "\n",
    "lines = []\n",
    "for i in range(len(data)):\n",
    "    d = data[i]\n",
    "    sentences = d[\"sentences\"]\n",
    "    labels = d[\"labels\"]\n",
    "    labelled_sentences = label_sentences(sentences, labels)\n",
    "    lines.extend(labelled_sentences)\n",
    "    \n",
    "# Shuffle the list\n",
    "import random\n",
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'__label__work', u'working uncertain estimates general changes numbers especially rates reported statistical_bulletin month periods small usually greater level explainable sampling variability')\n"
     ]
    }
   ],
   "source": [
    "corpus_fname = \"ons_labelled_corpus.txt\"\n",
    "print lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Data\n",
    "In order to test the accuracy of our model, we purposely keep part of the training data back as a validation dataset by splitting the corpus into two files, with extendions \".train\" and \".valid\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def write_corpus(corpus, prefix):\n",
    "    \"\"\"\n",
    "    Splits the corpus into training (.train) and validation (.valid) datasets\n",
    "    \"\"\"\n",
    "    random.shuffle(corpus)\n",
    "    \n",
    "    size_train = int(np.round(len(corpus) * (3./4.)))\n",
    "    train_corpus = corpus[:size_train]\n",
    "    valid_corpus = corpus[size_train:]\n",
    "    \n",
    "    extensions = [\"train\", \"valid\"]\n",
    "    corpa = [train_corpus, valid_corpus]\n",
    "    \n",
    "    for ext, corpus in zip(extensions, corpa):\n",
    "        modes = [\"labelled\", \"unlabelled\"]\n",
    "        fnames = [\"%s_%s.txt.%s\" % (prefix, mode, ext) for mode in modes]\n",
    "        for fname, mode in zip(fnames, modes):\n",
    "            with open(fname, \"w\") as f:\n",
    "                for line in corpus:\n",
    "                    label, text = line\n",
    "                    s = None\n",
    "                    if mode == \"labelled\":\n",
    "                        s = \"%s %s\" % (label, text)\n",
    "                    elif mode == \"unlabelled\":\n",
    "                        s = text\n",
    "                    if len(s) > 0:\n",
    "                        s = re.sub( '\\s+', ' ', s.encode(\"ascii\", \"ignore\") ).strip()\n",
    "                        f.write(\"%s\\n\" % s)\n",
    "\n",
    "# if os.path.isfile(corpus_fname) is False:\n",
    "write_corpus(lines, \"ons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fasttext (mode=skipgram) on ons_unlabelled.txt.train corpus..\n",
      "\n",
      "Training fasttext (mode=skipgram) on ons_unlabelled.txt.train corpus (without char n-grams)..\n",
      "\n",
      "Training word2vec on ons_unlabelled.txt.train corpus..\n",
      "\n",
      "Saved gensim model as ons_gs.vec\n"
     ]
    }
   ],
   "source": [
    "def which(file):\n",
    "    import os\n",
    "    for path in os.environ[\"PATH\"].split(os.pathsep):\n",
    "        if os.path.exists(os.path.join(path, file)):\n",
    "                return os.path.join(path, file)\n",
    "\n",
    "    return None\n",
    "\n",
    "def train_models(fastText_mode, corpus_file, output_name, models_dir):\n",
    "    # fastText training params\n",
    "    lr = 0.05\n",
    "    dim = 300\n",
    "    ws = 5\n",
    "    epoch = 5\n",
    "    minCount = 5\n",
    "    neg = 5\n",
    "    loss = 'ns'\n",
    "    t = 1e-4\n",
    "\n",
    "    from gensim.models import Word2Vec, KeyedVectors\n",
    "    from gensim.models.word2vec import Text8Corpus\n",
    "\n",
    "    # Same values as used for fastText training above\n",
    "    params = {\n",
    "        'alpha': lr,\n",
    "        'size': dim,\n",
    "        'window': ws,\n",
    "        'iter': epoch,\n",
    "        'min_count': minCount,\n",
    "        'sample': t,\n",
    "        'sg': 1,\n",
    "        'hs': 0,\n",
    "        'negative': neg\n",
    "    }\n",
    "\n",
    "    # Check for fasttext binary in path\n",
    "    fasttext = which(\"fasttext\")\n",
    "    if (fasttext is None):\n",
    "        raise Exception(\"Unable to locate fasttext binary in $PATH\")\n",
    "\n",
    "    # Generate the models\n",
    "\n",
    "    # fastText with ngrams\n",
    "    output_file = '{:s}_ft'.format(output_name)\n",
    "    print('Training fasttext (mode={:s}) on {:s} corpus..'.format(fastText_mode, corpus_file))\n",
    "    exe = \"{fasttext} {mode} -input {corpus_file} -output {output}  -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t}\"\n",
    "    exe = exe.format(fasttext=fasttext, mode=fastText_mode, corpus_file=corpus_file, output=models_dir+output_file, lr=lr, dim=dim, ws=ws, epoch=epoch, minCount=minCount, neg=neg, loss=loss, t=t)\n",
    "    os.system(exe)\n",
    "        \n",
    "    # fastText with NO ngrams\n",
    "    output_file = '{:s}_ft_no_ng'.format(output_name)\n",
    "    print('\\nTraining fasttext (mode={:s}) on {:s} corpus (without char n-grams)..'.format(fastText_mode, corpus_file))\n",
    "    exe = \"{fasttext} {mode} -input {corpus_file} -output {output}  -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t} -maxn 0\"\n",
    "    exe = exe.format(fasttext=fasttext, mode=fastText_mode, corpus_file=corpus_file, output=models_dir+output_file, lr=lr, dim=dim, ws=ws, epoch=epoch, minCount=minCount, neg=neg, loss=loss, t=t)\n",
    "    os.system(exe)\n",
    "        \n",
    "    # Word2Vec\n",
    "    output_file = '{:s}_gs'.format(output_name)\n",
    "    print('\\nTraining word2vec on {:s} corpus..'.format(corpus_file))\n",
    "    \n",
    "    # Text8Corpus class for reading space-separated words file\n",
    "    gs_model = Word2Vec(Text8Corpus(corpus_file), **params)\n",
    "    # Direct local variable lookup doesn't work properly with magic statements (%time)\n",
    "    gs_model.wv.save_word2vec_format(os.path.join(models_dir, '{:s}.vec'.format(output_file)))\n",
    "    print('\\nSaved gensim model as {:s}.vec'.format(output_file))\n",
    "    \n",
    "train_models(\"skipgram\", \"ons_unlabelled.txt.train\", \"ons\", \"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = None\n",
    "if os.path.isfile(\"models/ons_supervised.bin\"):\n",
    "    model = fastText.load_model(\"models/ons_supervised.bin\")\n",
    "else:\n",
    "    print \"Training\"\n",
    "    model = fastText.train_supervised(input=\"ons_labelled_corpus.txt.train\", label=\"__label__\",\\\n",
    "                              epoch=50, lr=1.0, wordNgrams=3, verbose=2, minCount=15,\\\n",
    "                              minCountLabel=15, thread=12, pretrainedVectors=\"models/ons_ft.vec\")\n",
    "    model.save_model(\"models/ons_supervised.bin\")\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "To test the model, we use the validation training set to try and predict the top 'k' labels. The output of the below test are the precision at k=1 (P@1) and the recall at k=1 (R@1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples= 52269\n",
      "P@1= 0.744934856225\n",
      "R@1= 0.227324210084\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "k=1\n",
    "N, P, R = model.test(\"%s.valid\" % corpus_fname, k)\n",
    "print \"Total number of samples=\", N\n",
    "print \"P@%d=\" % k, P\n",
    "print \"R@%d=\" % k, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the precision and recall at k=5 with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples= 52269\n",
      "P@5= 0.503594865025\n",
      "R@5= 0.768384671073\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "k=5\n",
    "N, P, R = model.test(\"%s.valid\" % corpus_fname, k)\n",
    "print \"Total number of samples=\", N\n",
    "print \"P@%d=\" % k, P\n",
    "print \"R@%d=\" % k, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdp:0.806220\n",
      "economy:0.106632\n",
      "oecd:0.045716\n",
      "growth:0.007496\n",
      "output:0.006851\n",
      "inflation:0.006261\n",
      "foreign:0.003135\n",
      "monetary:0.003119\n",
      "labour_market:0.002981\n",
      "trade:0.002403\n"
     ]
    }
   ],
   "source": [
    "# Example output\n",
    "k = 10\n",
    "label_prefix = \"__label__\"\n",
    "labels, probs = model.predict(\"UKs shortfall to Germany\", k)\n",
    "for label,prob in zip(labels, probs):\n",
    "    print \"%s:%f\" % (label.replace(label_prefix, \"\"), prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "In this notebook, we have successfully trained a model which can predict ONS keywords from raw text, using published articles and bulletins for supervised training. Such a model can be used to recommend keywords based on raw, human written, text input or even classify search terms into keyword categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
