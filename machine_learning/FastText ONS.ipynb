{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning of Keywords using fastText\n",
    "## Text Classification\n",
    "The goal of text classification is to assign documents (such as emails, posts, text messages, product reviews, etc...) to one or multiple categories. Such categories can be review scores, spam v.s. non-spam, or the language in which the document was typed. Nowadays, the dominant approach to build such classifiers is machine learning, that is learning classification rules from examples. In order to build such classifiers, we need labeled data, which consists of documents and their corresponding categories (or tags, or labels).\n",
    "\n",
    "As an example, we build a classifier which automatically classifies ONS publications by their supplied keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fastText\n",
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.isdir(\"./models/\") is False:\n",
    "    !mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpa\n",
    "To create the text corpus, we load in articles and bulletins published on the ONS website. The final corpus consists of sentences found in the pages summaries and markdown sections, tagged by the keywords provided with each page.\n",
    "\n",
    "For the purpose of this notebook, we have stored all artices and bulletins in a local mongoDB database for painless retrieval. Lets define some functions for loading the pages as json below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMongoDBClient(mongoUrl, port):\n",
    "    from pymongo import MongoClient\n",
    "    return MongoClient(mongoUrl, port)\n",
    "\n",
    "def load_pages(use_mongo=True):\n",
    "    # Load pages from disk/mongo\n",
    "    pages = None\n",
    "    print \"Loading pages from \" + \"mongoDB\" if use_mongo else \"filesystem\"\n",
    "    if (use_mongo):\n",
    "        mongoClient = getMongoDBClient(\"localhost\", 27017)\n",
    "        collection = mongoClient.local.pages\n",
    "\n",
    "        query = {\n",
    "            \"sections\": {\n",
    "                \"$exists\": True\n",
    "            }\n",
    "        }\n",
    "        cursor = collection.find(query)\n",
    "\n",
    "        pages = []\n",
    "        for doc in cursor:\n",
    "            pages.append(doc)\n",
    "    else:\n",
    "        # Read from filesystem\n",
    "        from modules.ONS.file_scanner import FileScanner\n",
    "        scanner = FileScanner()\n",
    "        pages = scanner.load_pages()\n",
    "    print \"Done\"\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pages from mongoDB\n",
      "Done\n",
      "Loaded 1960 pages\n"
     ]
    }
   ],
   "source": [
    "pages = load_pages(use_mongo=True)\n",
    "print \"Loaded %d pages\" % (len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test processing\n",
    "To give our model the best chance of accurate classifications, we should clean up the raw text keywords to remove things like stop words etc. Below we define some basic utility functions to clean raw text, then process the pages we just loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_stopwords():\n",
    "    return set(stopwords.words('english'))  # nltk stopwords list\n",
    "\n",
    "def get_bigram(train_texts):\n",
    "    import gensim\n",
    "    bigram = gensim.models.Phrases(train_texts)  # for bigram collocation detection\n",
    "    return bigram\n",
    "\n",
    "def build_texts_from_file(fname):\n",
    "    import gensim\n",
    "    \"\"\"\n",
    "    Function to build tokenized texts from file\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    fname: File to be read\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    yields preprocessed line\n",
    "    \"\"\"\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield gensim.utils.simple_preprocess(line, deacc=True, min_len=3)\n",
    "\n",
    "def build_texts_as_list_from_file(fname):\n",
    "    return list(build_texts_from_file(fname))\n",
    "\n",
    "def build_texts(texts):\n",
    "    import gensim\n",
    "    \"\"\"\n",
    "    Function to build tokenized texts from file\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    fname: File to be read\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    yields preprocessed line\n",
    "    \"\"\"\n",
    "    for line in texts:\n",
    "        yield gensim.utils.simple_preprocess(line, deacc=True, min_len=3)\n",
    "\n",
    "def build_texts_as_list(texts):\n",
    "    return list(build_texts(texts))\n",
    "\n",
    "def process_texts(texts, stops=get_stopwords()):\n",
    "    \"\"\"\n",
    "    Function to process texts. Following are the steps we take:\n",
    "    \n",
    "    1. Stopword Removal.\n",
    "    2. Collocation detection.\n",
    "    3. Lemmatization (not stem since stemming can reduce the interpretability).\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    texts: Tokenized texts.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    texts: Pre-processed tokenized texts.\n",
    "    \"\"\"\n",
    "    bigram = get_bigram(texts)\n",
    "\n",
    "    texts = [[word for word in line if word not in stops] for line in texts]\n",
    "    texts = [bigram[line] for line in texts]\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    texts = [[word for word in lemmatizer.lemmatize(' '.join(line), pos='v').split()] for line in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "ons_ft = gensim.models.KeyedVectors.load_word2vec_format(\"models/ons_ft.vec\")\n",
    "\n",
    "def parse_corpus(content, stop=None):\n",
    "    # Convert 'content' into properly formatted text corpus with stop word removal\n",
    "\n",
    "#     print \"Parsing text corpus...\"\n",
    "\n",
    "    stop = get_stopwords()\n",
    "    texts = build_texts_as_list(content)\n",
    "    texts = process_texts(texts, stops=stop)\n",
    "\n",
    "#     print \"Done\"\n",
    "    return texts\n",
    "\n",
    "def generate_corpus(pages):\n",
    "    # Gather some human written text into 'content'\n",
    "    import markdown\n",
    "    from string import punctuation\n",
    "    from bs4 import BeautifulSoup\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    print \"Generating text corpus...\"\n",
    "\n",
    "    pattern = \"[, \\-!?:]+\"\n",
    "\n",
    "    fix_encoding = lambda s: s.decode('utf8', 'ignore')\n",
    "\n",
    "    def strip_punctuation(s):\n",
    "        return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "    def markdown_to_text(md):\n",
    "        extensions = ['extra', 'smarty']\n",
    "        html = markdown.markdown(md, extensions=extensions, output_format='html5')\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        return soup.text\n",
    "    \n",
    "    def get_keywords(description):\n",
    "        keywords = []\n",
    "        for keyword in description[\"keywords\"]:\n",
    "            keyword = keyword.lower().strip().replace(\" \" , \"_\")\n",
    "            if \",\" in keyword:\n",
    "                keywords.extend([k.lower().strip().replace(\" \" , \"_\") for k in keyword.split(\",\")])\n",
    "            else:\n",
    "                keywords.append(keyword)\n",
    "        return keywords\n",
    "    \n",
    "    def proceed(page):\n",
    "        if \"description\" in page \\\n",
    "                and \"keywords\" in page[\"description\"] \\\n",
    "                and len(page[\"description\"][\"keywords\"]) > 0:\n",
    "#             for keyword in get_keywords(page[\"description\"]):\n",
    "#                 valid = re.match('^[\\w-]+$', keyword) is not None\n",
    "#                 if not valid:\n",
    "#                     return False\n",
    "#                 if keyword not in ons_ft.wv.vocab:\n",
    "#                     return False\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def process_text(text, corpus, min_sent_length=4):\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            if len(tokens) >= min_sent_length:\n",
    "                doc = {\"text\": sentence, \"keywords\": keywords}        \n",
    "                corpus.append(doc)\n",
    "                \n",
    "    from progressbar import ProgressBar\n",
    "    pbar = ProgressBar()\n",
    "\n",
    "    corpus = []\n",
    "    for page in pbar(pages):\n",
    "        if proceed(page):\n",
    "            description = page[\"description\"]\n",
    "            keywords = get_keywords(description)\n",
    "            \n",
    "            text = description[\"title\"]\n",
    "#             process_text(text, corpus)\n",
    "                \n",
    "            if \"summary\" in description:\n",
    "                text = description[\"summary\"]\n",
    "                process_text(text, corpus)\n",
    "            if \"sections\" in page:\n",
    "                for section in page[\"sections\"]:\n",
    "                    if \"title\" in section:\n",
    "                        text = section[\"title\"]\n",
    "                        process_text(text, corpus)\n",
    "                    if \"markdown\" in section:\n",
    "                        text = markdown_to_text(section[\"markdown\"])\n",
    "                        process_text(text, corpus)\n",
    "                \n",
    "            \n",
    "    print \"Done\"\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "232647\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "do_load=False\n",
    "\n",
    "if do_load:\n",
    "    with open(\"./raw_corpus.p\", \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "else:\n",
    "    corpus = generate_corpus(pages)\n",
    "    pickle.dump(np.array(corpus), open(\"./raw_corpus.p\", \"wb\"))\n",
    "print len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the training set\n",
    "The final step is to write the training data out in the correct format. We prepend a prefix to each label, so the classifier knows how to identify them aside from the raw text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# random.shuffle(corpus)\n",
    "\n",
    "# stops = []\n",
    "\n",
    "# token_threshold = 20\n",
    "\n",
    "# size_train = int(np.round(len(corpus) * (3./4.)))\n",
    "# train_corpus = corpus[:size_train]\n",
    "# valid_corpus = corpus[size_train:]\n",
    "\n",
    "# with open(\"train.txt\", \"w\") as f:\n",
    "#     for i in range(len(train_corpus)):\n",
    "#         text = train_corpus[i][\"text\"].strip()\n",
    "#         text = process_texts([text.split()], stops=stops)[0]\n",
    "        \n",
    "#         ntokens = len(text)\n",
    "        \n",
    "#         if ntokens > token_threshold:\n",
    "#             text = \" \".join(text)\n",
    "#             if text.endswith(\".\") is False:\n",
    "#                 text += \" <eos>\"\n",
    "#             else:\n",
    "#                 text = \"%s <eos>\" % text[:-1]\n",
    "\n",
    "#             s = re.sub( '\\s+', ' ', text.encode(\"ascii\", \"ignore\") ).strip()\n",
    "#             if i < len(train_corpus) - 1:\n",
    "#                 s = \"%s\\n\" % s\n",
    "#             f.write(s)\n",
    "        \n",
    "\n",
    "# with open(\"valid.txt\", \"w\") as f:\n",
    "#     for i in range(len(valid_corpus)):\n",
    "#         text = valid_corpus[i][\"text\"].strip()\n",
    "#         text = process_texts([text.split()], stops=stops)[0]\n",
    "        \n",
    "#         ntokens = len(text)\n",
    "        \n",
    "#         if ntokens > token_threshold:\n",
    "#             text = \" \".join(text)\n",
    "#             if text.endswith(\".\") is False:\n",
    "#                 text += \" <eos>\"\n",
    "#             else:\n",
    "#                 text = \"%s <eos>\" % text[:-1]\n",
    "\n",
    "#             s = re.sub( '\\s+', ' ', text.encode(\"ascii\", \"ignore\") ).strip()\n",
    "#             if i < len(valid_corpus) - 1:\n",
    "#                 s = \"%s\\n\" % s\n",
    "#             f.write(s)\n",
    "        \n",
    "# !cp train.txt test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "stops = []\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "\n",
    "with open(\"./ons_corpus.txt\", \"w\") as f:\n",
    "    for d in pbar(corpus):\n",
    "        text = d[\"text\"].strip()\n",
    "        s = re.sub( '\\s+', ' ', text.encode(\"ascii\", \"ignore\") ).strip()\n",
    "        s = s.split()\n",
    "        if len(s) > 10:\n",
    "            s = process_texts([s], stops=stops)[0]\n",
    "            s = \" \".join(s)\n",
    "            \n",
    "            if s.endswith(\".\") is False:\n",
    "                s = s + \".\"\n",
    "        \n",
    "            f.write(s + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 232647 items in corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sullid/anaconda2/lib/python2.7/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "# stops = get_stopwords()\n",
    "# stops.append(\".\")\n",
    "stops = []\n",
    "\n",
    "def label_sentence(text, labels, label_prefix=\"__label__\"):\n",
    "    \n",
    "    try:\n",
    "        labels = filter(None, process_texts([labels], stops=stops))[0]\n",
    "        labels = [l for l in labels if l in ons_ft.wv.vocab]\n",
    "\n",
    "        if len(labels) > 0:\n",
    "            labels = set( [\"%s%s\" % (label_prefix, l) for l in labels] )\n",
    "            joined_labels = \" \".join(labels)\n",
    "\n",
    "            text = process_texts([text.split()], stops=stops)[0]\n",
    "            text = \" \".join(text)\n",
    "            if text.endswith(\".\") is False:\n",
    "                text += \".\"\n",
    "\n",
    "            return \"%s %s\" % (joined_labels, text)\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "print \"Processing %d items in corpus\" % len(corpus)\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "\n",
    "lines = []\n",
    "for d in pbar(corpus):\n",
    "    text = d[\"text\"].strip()\n",
    "    keywords = d[\"keywords\"]\n",
    "    line = label_sentence(text, keywords)\n",
    "    if line is not None:\n",
    "        lines.append(line.lower())\n",
    "    \n",
    "# Shuffle the list\n",
    "import random\n",
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222726\n",
      "__label__debt_burden __label__pension individuals aged 35 to 44 indicating that they have the heaviest debt burden (22.8%) and the highest reports of somewhat of a burden (38.9%).\n"
     ]
    }
   ],
   "source": [
    "print len(lines)\n",
    "print lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Data\n",
    "In order to test the accuracy of our model, we purposely keep part of the training data back as a validation dataset by splitting the corpus into two files, with extendions \".train\" and \".valid\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167044\n",
      "55682\n",
      "Corpa written to file.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def write_corpus(full_corpus, prefix):\n",
    "    \"\"\"\n",
    "    Splits the corpus into training (.train) and validation (.valid) datasets\n",
    "    \"\"\"    \n",
    "    size_train = int(np.round(len(full_corpus) * (3./4.)))\n",
    "    train_corpus = full_corpus[:size_train]\n",
    "    valid_corpus = full_corpus[size_train:]\n",
    "    \n",
    "    print len(train_corpus)\n",
    "    print len(valid_corpus)\n",
    "    \n",
    "    extensions = [\".train\", \".valid\", \"\"]\n",
    "    corpa = [train_corpus, valid_corpus, full_corpus]\n",
    "    \n",
    "    for ext, cps in zip(extensions, corpa):\n",
    "        fname = \"%s_labelled.txt%s\" % (prefix, ext)\n",
    "        with open(fname, \"w\") as f:\n",
    "            for s in cps:\n",
    "                if len(s) > 0:\n",
    "                    s = re.sub( '\\s+', ' ', s.encode(\"ascii\", \"ignore\") ).strip()\n",
    "                    f.write(\"%s\\n\" % s)\n",
    "\n",
    "# if os.path.isfile(corpus_fname) is False:\n",
    "write_corpus(lines, \"ons\")\n",
    "print \"Corpa written to file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import fastText\n",
    "print \"Training...\"\n",
    "model = fastText.train_supervised(input=\"ons_labelled.txt.train\", label=\"__label__\",\\\n",
    "                          dim=300, epoch=1000, wordNgrams=2, verbose=2, minCount=15,\\\n",
    "                          minCountLabel=5, lr=0.1, neg=10, thread=16, loss=\"ns\", t=1e-5)\n",
    "model.save_model(\"models/ons_supervised.bin\")\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "To test the model, we use the validation training set to try and predict the top 'k' labels. The output of the below test are the precision at k=1 (P@1) and the recall at k=1 (R@1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fastText\n",
    "\n",
    "model = fastText.load_model(\"models/ons_supervised.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "k=1\n",
    "N, P, R = model.test(\"ons_labelled.txt.valid\", k)\n",
    "print \"Total number of samples=\", N\n",
    "print \"P@%d=\" % k, P\n",
    "print \"R@%d=\" % k, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the precision and recall at k=5 with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "k=5\n",
    "N, P, R = model.test(\"ons_labelled.txt.valid\", k)\n",
    "print \"Total number of samples=\", N\n",
    "print \"P@%d=\" % k, P\n",
    "print \"R@%d=\" % k, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "k=10\n",
    "N, P, R = model.test(\"ons_labelled.txt.valid\", k)\n",
    "print \"Total number of samples=\", N\n",
    "print \"P@%d=\" % k, P\n",
    "print \"R@%d=\" % k, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example output\n",
    "k = 10\n",
    "label_prefix = \"__label__\"\n",
    "labels, probs = model.predict(\"UKs shortfall to Germany\".lower(), k)\n",
    "for label,prob in zip(labels, probs):\n",
    "    print \"%s:%f\" % (label.replace(label_prefix, \"\"), prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduce memory usage of model and save\n",
    "\n",
    "# Use PCA to reduce size of input and output matricies\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=50)\n",
    "\n",
    "im = model.get_input_matrix()\n",
    "om = model.get_output_matrix()\n",
    "\n",
    "pca_fit_im = pca.fit(im)\n",
    "im_transformed = pca_fit_im.transform(im)\n",
    "\n",
    "pca_fit_om = pca.fit(om)\n",
    "om_transformed = pca_fit_om.transform(om)\n",
    "\n",
    "for fname, arr in zip([\"ons_supervised_quantized.bin.im_reduced\", \"ons_supervised_quantized.bin.om_reduced\"], [im_transformed, om_transformed]):\n",
    "    with open(fname, \"wb\") as f:\n",
    "        arr.dump(f)\n",
    "\n",
    "model.quantize()\n",
    "model.save_model(\"models/ons_supervised_quantized.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"ons_supervised_quantized.bin.im_pca\", \"wb\") as f:\n",
    "    pickle.dump(pca_fit_im, f)\n",
    "    \n",
    "with open(\"ons_supervised_quantized.bin.om_pca\", \"wb\") as f:\n",
    "    pickle.dump(pca_fit_om, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "In this notebook, we have successfully trained a model which can predict ONS keywords from raw text, using published articles and bulletins for supervised training. Such a model can be used to recommend keywords based on raw, human written, text input or even classify search terms into keyword categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fastText\n",
    "\n",
    "model = fastText.load_model(\"models/ons_supervised.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = model.get_labels()\n",
    "norm = lambda vec: vec / np.linalg.norm(vec)\n",
    "\n",
    "om = model.get_output_matrix()\n",
    "word_vectors = np.zeros(om.shape)\n",
    "for i in range(len(om)):\n",
    "    word_vectors[i] = norm(om[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, u'__label__gdp')\n",
      "(0.5799029683224861, u'__label__unit')\n",
      "(0.53219564271997, u'__label__investment')\n",
      "(0.527457115069589, u'__label__sectors')\n",
      "(0.507968012949438, u'__label__brazil')\n",
      "(0.5021528824242844, u'__label__output')\n",
      "(0.5017086959446281, u'__label__growths')\n",
      "(0.4884507690957247, u'__label__annual')\n",
      "(0.4837299025589181, u'__label__international_comparisons')\n",
      "(0.4527872163462047, u'__label__recreation')\n",
      "(0.4477983338894497, u'__label__per')\n",
      "(0.4476581784113353, u'__label__sport')\n",
      "(0.4467532841136419, u'__label__hour')\n",
      "(0.4461745364758096, u'__label__mbs')\n",
      "(0.4304655989237989, u'__label__labour')\n",
      "(0.41896423913067865, u'__label__trade')\n",
      "(0.41696726450555627, u'__label__oecd')\n",
      "(0.3973072457027024, u'__label__precious_metals')\n",
      "(0.39129522577894055, u'__label__valuables')\n",
      "(0.39017144490078787, u'__label__gcf')\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors\n",
    "    :param vec1:\n",
    "    :param vec2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return cos_sim\n",
    "\n",
    "def cosine_sim2(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors\n",
    "    :param vec1:\n",
    "    :param vec2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cos_sim = [np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)) for v1,v2 in zip(vec1, vec2)]\n",
    "    return np.array(cos_sim)\n",
    "\n",
    "ix_gdp = np.where(np.array(words) == \"__label__gdp\")\n",
    "vec1 = norm(word_vectors[ix_gdp])\n",
    "d = np.zeros(len(word_vectors))\n",
    "for i,vec in enumerate(word_vectors):\n",
    "    d[i] = cosine_sim(vec1, vec)\n",
    "#     d[i] = euclidean(vec1, vec)\n",
    "    \n",
    "top_n = 20\n",
    "top_labels = sorted(zip(d, words), reverse=True)[:top_n]\n",
    "for top_label in top_labels:\n",
    "    print top_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing K-Means...\n",
      "Done\n",
      "Computing Euclidean distances...\n",
      "Done\n",
      "Computing pairwise distances...\n",
      "Computing percentage of variance explained...\n",
      "Plotting...\n",
      "Minimum number of clusters =  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sullid/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:54: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in greater\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAHmCAYAAABqChckAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b15dbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def new_euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False): \n",
    "    return cosine_sim2(X, Y)\n",
    "\n",
    "# monkey patch (ensure cosine dist function is used)\n",
    "from sklearn.cluster import k_means_\n",
    "k_means_.euclidean_distances = new_euclidean_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "\n",
    "def eblow(data, n):\n",
    "    print \"Performing K-Means...\"\n",
    "    kMeansVar = [KMeans(n_clusters=k, random_state=0).fit(data) for k in range(1, n)]\n",
    "    print \"Done\"\n",
    "    centroids = [X.cluster_centers_ for X in kMeansVar]\n",
    "    print \"Computing Euclidean distances...\"\n",
    "    k_euclid = [cdist(data, cent) for cent in centroids]\n",
    "    dist = [np.min(ke, axis=1) for ke in k_euclid]\n",
    "    avgWithinSS = [sum(d)/data.shape[0] for d in dist]\n",
    "    print \"Done\"\n",
    "    wcss = [sum(d**2) for d in dist]\n",
    "    print \"Computing pairwise distances...\"\n",
    "    pair_dist = pdist(data)\n",
    "    print \"Computing percentage of variance explained...\"\n",
    "    tss = sum(pair_dist**2)/data.shape[0]\n",
    "    bss = tss - wcss\n",
    "    percentage_variance = (bss/tss)*100.0\n",
    "    \n",
    "    K = range(1,n)\n",
    "    seg_threshold = 0.95 #Set this to your desired target\n",
    "\n",
    "    #The angle between three points\n",
    "    def segments_gain(p1, v, p2):\n",
    "        vp1 = np.linalg.norm(p1 - v)\n",
    "        vp2 = np.linalg.norm(p2 - v)\n",
    "        p1p2 = np.linalg.norm(p1 - p2)\n",
    "        return np.arccos((vp1**2 + vp2**2 - p1p2**2) / (2 * vp1 * vp2)) / np.pi\n",
    "\n",
    "    #Normalize the data\n",
    "    criterion = np.array(avgWithinSS)\n",
    "    criterion = (criterion - criterion.min()) / (criterion.max() - criterion.min())\n",
    "\n",
    "    #Compute the angles\n",
    "    seg_gains = np.array([0, ] + [segments_gain(*\n",
    "            [np.array([K[j], criterion[j]]) for j in range(i-1, i+2)]\n",
    "        ) for i in range(len(K) - 2)] + [np.nan, ])\n",
    "\n",
    "    #Get the first index satisfying the threshold\n",
    "    kIdx = np.argmax(seg_gains > seg_threshold)\n",
    "    \n",
    "    print \"Plotting...\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(K[kIdx], percentage_variance[kIdx], marker='o', markersize=12, \n",
    "        markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "    \n",
    "    ax.scatter(K, percentage_variance, color='b')\n",
    "    ax.plot(K, percentage_variance, color='b')\n",
    "    \n",
    "    ax.set_xlabel(\"Number of Clusters\", fontsize=12)\n",
    "    ax.set_ylabel(\"Percentage of Variance Explained [%]\", fontsize=12)\n",
    "    \n",
    "    print \"Minimum number of clusters = \", K[kIdx]\n",
    "    \n",
    "    return kMeansVar\n",
    "    \n",
    "kMeansVar = eblow(word_vectors, 31)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "def matplotlib_to_plotly(cmap, pl_entries):\n",
    "    h = 1.0/(pl_entries-1)\n",
    "    pl_colorscale = []\n",
    "    \n",
    "    for k in range(pl_entries):\n",
    "        C = map(np.uint8, np.array(cmap(k*h)[:3])*255)\n",
    "        pl_colorscale.append([k*h, 'rgb'+str((C[0], C[1], C[2]))])\n",
    "        \n",
    "    return pl_colorscale\n",
    "\n",
    "def ncols(NUM_COLORS, cmap='gist_rainbow'):\n",
    "    import matplotlib.cm as mplcm\n",
    "    import matplotlib.colors as colors\n",
    "    \n",
    "    cm = plt.get_cmap(cmap)\n",
    "    cNorm  = colors.Normalize(vmin=0, vmax=NUM_COLORS-1)\n",
    "    scalarMap = mplcm.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "    return scalarMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798 798\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Sully0190/6.embed\" height=\"900px\" width=\"900px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "from sklearn import decomposition\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors\n",
    "    :param vec1:\n",
    "    :param vec2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return cos_sim\n",
    "\n",
    "kOpt = 10\n",
    "pca3 = decomposition.PCA(n_components=3)\n",
    "pca_fit3 = pca3.fit(word_vectors)\n",
    "X = pca_fit3.transform(word_vectors)\n",
    "words = model.get_labels()\n",
    "\n",
    "print len(words), len(word_vectors)\n",
    "\n",
    "NUM_COLORS = kOpt+1\n",
    "scalarMap = ncols(NUM_COLORS, cmap=\"jet\")\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=1,\n",
    "                          print_grid=False,\n",
    "                          specs=[[{'is_3d': True}]])\n",
    "\n",
    "def similarity(word1, word2, model):\n",
    "    w1 = model.get_word_vector(word1)\n",
    "    w2 = model.get_word_vector(word2)\n",
    "    return cosine_sim(w1, w2)\n",
    "\n",
    "scene = dict(\n",
    "    camera = dict(\n",
    "    up=dict(x=0, y=0, z=1),\n",
    "    center=dict(x=0, y=0, z=0),\n",
    "    eye=dict(x=2.5, y=0.1, z=0.1)\n",
    "    ),\n",
    "    xaxis=dict(title='pc1',\n",
    "        gridcolor='rgb(255, 255, 255)',\n",
    "        zerolinecolor='rgb(255, 255, 255)',\n",
    "        showbackground=True,\n",
    "        backgroundcolor='rgb(230, 230,230)',\n",
    "        showticklabels=False, ticks=''\n",
    "    ),\n",
    "    yaxis=dict(title='pc2',\n",
    "        gridcolor='rgb(255, 255, 255)',\n",
    "        zerolinecolor='rgb(255, 255, 255)',\n",
    "        showbackground=True,\n",
    "        backgroundcolor='rgb(230, 230,230)',\n",
    "        showticklabels=False, ticks=''\n",
    "    ),\n",
    "    zaxis=dict(title='pc3',\n",
    "        gridcolor='rgb(255, 255, 255)',\n",
    "        zerolinecolor='rgb(255, 255, 255)',\n",
    "        showbackground=True,\n",
    "        backgroundcolor='rgb(230, 230,230)',\n",
    "        showticklabels=False, ticks=''\n",
    "    )\n",
    ")\n",
    "\n",
    "name,est = ('k_means', KMeans(n_clusters=kOpt, random_state=0).fit(word_vectors))\n",
    "vector_idx = est.fit_predict(word_vectors)\n",
    "\n",
    "cols = [scalarMap.to_rgba(i) for i in range(NUM_COLORS)]\n",
    "\n",
    "# Add cluster centroids\n",
    "centroids = pca_fit3.transform(est.cluster_centers_)\n",
    "# centroidNames = [\"Centroid %d\" % (i+1) for i in range(kOpt)]\n",
    "\n",
    "for icluster in range(kOpt):\n",
    "    idx = np.where(vector_idx == icluster)[0]\n",
    "    labels = est.labels_[idx]\n",
    "    data = X[idx]\n",
    "    \n",
    "    vec = est.cluster_centers_[icluster]\n",
    "    name=\"Centroid %d\" % icluster\n",
    "\n",
    "    text = []\n",
    "    for ix in idx:\n",
    "        text.append(words[ix])\n",
    "    pcols = [cols[icluster]] * len(idx)\n",
    "\n",
    "    trace = go.Scatter3d(x=data[:, 0], y=data[:, 1], z=data[:, 2],\n",
    "                         name=name,\n",
    "                         text=text,\n",
    "                         showlegend=True,\n",
    "                         mode='markers',\n",
    "                         marker=dict(\n",
    "                                color=pcols[:],\n",
    "                                line=dict(color='black', width=1)\n",
    "        ))\n",
    "    fig.append_trace(trace, 1, 1)\n",
    "    \n",
    "    centroidsTrace = go.Scatter3d(x=centroids[icluster, 0], y=centroids[icluster, 1], z=centroids[icluster, 2],\n",
    "                         name=name,\n",
    "                         text=\"Cluster %d\" % (icluster+1),\n",
    "                         showlegend=False,\n",
    "                         mode='markers',\n",
    "                         marker=dict(\n",
    "                                color=\"black\",\n",
    "                                line=dict(color='black', width=1)\n",
    "        ))\n",
    "    fig.append_trace(centroidsTrace, 1, 1)\n",
    "\n",
    "fig['layout'].update(height=900, width=900,\n",
    "                     margin=dict(l=10,r=10))\n",
    "\n",
    "fig['layout']['scene1'].update(scene)\n",
    "fig['layout']['scene2'].update(scene)\n",
    "fig['layout']['scene3'].update(scene)\n",
    "fig['layout']['scene4'].update(scene)\n",
    "fig['layout']['scene5'].update(scene)\n",
    "\n",
    "# Use py.iplot() for IPython notebook\n",
    "py.iplot(fig, filename='3d ONS point clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(array([  1,   5,   6,  10,  11,  12,  13,  48,  58,  69,  79,  87, 107,\n",
      "       135, 149, 167, 178, 183, 201, 208, 210, 211, 219, 256, 261, 292,\n",
      "       303, 308, 319, 368, 369, 405, 409, 422, 423, 469, 514, 549, 592,\n",
      "       600, 601, 602, 603, 618, 628, 664, 665, 666, 669, 690, 732, 733,\n",
      "       736, 742, 747, 749, 769, 771, 776]),)\n",
      "59 59\n"
     ]
    }
   ],
   "source": [
    "ix = np.where(np.array(words) == \"__label__gdp\")\n",
    "\n",
    "gdp_cluster = vector_idx[ix][0]\n",
    "print gdp_cluster\n",
    "\n",
    "cluster_words_ix = np.where(vector_idx == gdp_cluster)\n",
    "print cluster_words_ix\n",
    "cluster_words = np.array(words)[cluster_words_ix]\n",
    "cluster_vecs = np.array(word_vectors)[cluster_words_ix]\n",
    "print len(cluster_words), len(cluster_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__gdp\n",
      "__label__wages\n",
      "__label__inflation\n",
      "__label__work\n",
      "__label__claimants\n",
      "__label__jobless\n",
      "__label__salaries\n",
      "__label__productivity\n",
      "__label__output\n",
      "__label__capital\n",
      "__label__income\n",
      "__label__qali\n",
      "__label__trade\n",
      "__label__students\n",
      "__label__healthcare\n",
      "__label__growth\n",
      "__label__mfp\n",
      "__label__housing\n",
      "__label__industrial\n",
      "__label__efficiency\n",
      "__label__building\n",
      "__label__vics\n",
      "__label__nhs\n",
      "__label__labour\n",
      "__label__commercial\n",
      "__label__stock\n",
      "__label__tables\n",
      "__label__opss\n",
      "__label__real_wages\n",
      "__label__ppp\n",
      "__label__oecd\n",
      "__label__unit\n",
      "__label__imputed_rental\n",
      "__label__flow\n",
      "__label__asset\n",
      "__label__leps\n",
      "__label__recession\n",
      "__label__living_standards\n",
      "__label__economic_downturn\n",
      "__label__solow\n",
      "__label__purchasing_power\n",
      "__label__employee\n",
      "__label__deflators\n",
      "__label__hospitals\n",
      "__label__workers\n",
      "__label__colleges\n",
      "__label__schools\n",
      "__label__fhs\n",
      "__label__teachers\n",
      "__label__awe\n",
      "__label__sub_regional\n",
      "__label__urban\n",
      "__label__devolution\n",
      "__label__taxes\n",
      "__label__per\n",
      "__label__hour\n",
      "__label__wage\n",
      "__label__preliminary_estimate\n",
      "__label__real_awe\n"
     ]
    }
   ],
   "source": [
    "for word in cluster_words:\n",
    "    print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
