{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning of Keywords using fastText\n",
    "## Text Classification\n",
    "The goal of text classification is to assign documents (such as emails, posts, text messages, product reviews, etc...) to one or multiple categories. Such categories can be review scores, spam v.s. non-spam, or the language in which the document was typed. Nowadays, the dominant approach to build such classifiers is machine learning, that is learning classification rules from examples. In order to build such classifiers, we need labeled data, which consists of documents and their corresponding categories (or tags, or labels).\n",
    "\n",
    "As an example, we build a classifier which automatically classifies ONS publications by their supplied keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fastText\n",
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.isdir(\"./models/\") is False:\n",
    "    !mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpa\n",
    "To create the text corpus, we load in articles and bulletins published on the ONS website. The final corpus consists of sentences found in the pages summaries and markdown sections, tagged by the keywords provided with each page.\n",
    "\n",
    "For the purpose of this notebook, we have stored all artices and bulletins in a local mongoDB database for painless retrieval. Lets define some functions for loading the pages as json below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMongoDBClient(mongoUrl, port):\n",
    "    from pymongo import MongoClient\n",
    "    return MongoClient(mongoUrl, port)\n",
    "\n",
    "def load_pages(use_mongo=True):\n",
    "    # Load pages from disk/mongo\n",
    "    pages = None\n",
    "    print \"Loading pages from \" + \"mongoDB\" if use_mongo else \"filesystem\"\n",
    "    if (use_mongo):\n",
    "        mongoClient = getMongoDBClient(\"localhost\", 27017)\n",
    "        collection = mongoClient.local.pages\n",
    "\n",
    "        query = {\n",
    "            \"sections\": {\n",
    "                \"$exists\": True\n",
    "            }\n",
    "        }\n",
    "        cursor = collection.find(query)\n",
    "\n",
    "        pages = []\n",
    "        for doc in cursor:\n",
    "            pages.append(doc)\n",
    "    else:\n",
    "        # Read from filesystem\n",
    "        from modules.ONS.file_scanner import FileScanner\n",
    "        scanner = FileScanner()\n",
    "        pages = scanner.load_pages()\n",
    "    print \"Done\"\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = load_pages(use_mongo=True)\n",
    "print \"Loaded %d pages\" % (len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test processing\n",
    "To give our model the best chance of accurate classifications, we should clean up the raw text keywords to remove things like stop words etc. Below we define some basic utility functions to clean raw text, then process the pages we just loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_stopwords():\n",
    "    return set(stopwords.words('english'))  # nltk stopwords list\n",
    "\n",
    "def get_bigram(train_texts):\n",
    "    import gensim\n",
    "    bigram = gensim.models.Phrases(train_texts)  # for bigram collocation detection\n",
    "    return bigram\n",
    "\n",
    "def build_texts_from_file(fname):\n",
    "    import gensim\n",
    "    \"\"\"\n",
    "    Function to build tokenized texts from file\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    fname: File to be read\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    yields preprocessed line\n",
    "    \"\"\"\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield gensim.utils.simple_preprocess(line, deacc=True, min_len=3)\n",
    "\n",
    "def build_texts_as_list_from_file(fname):\n",
    "    return list(build_texts_from_file(fname))\n",
    "\n",
    "def build_texts(texts):\n",
    "    import gensim\n",
    "    \"\"\"\n",
    "    Function to build tokenized texts from file\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    fname: File to be read\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    yields preprocessed line\n",
    "    \"\"\"\n",
    "    for line in texts:\n",
    "        yield gensim.utils.simple_preprocess(line, deacc=True, min_len=3)\n",
    "\n",
    "def build_texts_as_list(texts):\n",
    "    return list(build_texts(texts))\n",
    "\n",
    "def process_texts(texts, stops=get_stopwords()):\n",
    "    \"\"\"\n",
    "    Function to process texts. Following are the steps we take:\n",
    "    \n",
    "    1. Stopword Removal.\n",
    "    2. Collocation detection.\n",
    "    3. Lemmatization (not stem since stemming can reduce the interpretability).\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    texts: Tokenized texts.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    texts: Pre-processed tokenized texts.\n",
    "    \"\"\"\n",
    "    bigram = get_bigram(texts)\n",
    "\n",
    "    texts = [[word for word in line if word not in stops] for line in texts]\n",
    "    texts = [bigram[line] for line in texts]\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    texts = [[word for word in lemmatizer.lemmatize(' '.join(line), pos='v').split()] for line in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "ons_ft = gensim.models.KeyedVectors.load_word2vec_format(\"models/ons_ft.vec\")\n",
    "\n",
    "def parse_corpus(content, stop=None):\n",
    "    # Convert 'content' into properly formatted text corpus with stop word removal\n",
    "\n",
    "#     print \"Parsing text corpus...\"\n",
    "\n",
    "    stop = get_stopwords()\n",
    "    texts = build_texts_as_list(content)\n",
    "    texts = process_texts(texts, stops=stop)\n",
    "\n",
    "#     print \"Done\"\n",
    "    return texts\n",
    "\n",
    "def generate_corpus(pages):\n",
    "    # Gather some human written text into 'content'\n",
    "    import markdown\n",
    "    from string import punctuation\n",
    "    from bs4 import BeautifulSoup\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    print \"Generating text corpus...\"\n",
    "\n",
    "    pattern = \"[, \\-!?:]+\"\n",
    "\n",
    "    fix_encoding = lambda s: s.decode('utf8', 'ignore')\n",
    "\n",
    "    def strip_punctuation(s):\n",
    "        return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "    def markdown_to_text(md):\n",
    "        extensions = ['extra', 'smarty']\n",
    "        html = markdown.markdown(md, extensions=extensions, output_format='html5')\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        return soup.text\n",
    "    \n",
    "    def get_keywords(description):\n",
    "        keywords = []\n",
    "        for keyword in description[\"keywords\"]:\n",
    "            keyword = keyword.lower().strip().replace(\" \" , \"_\")\n",
    "            if \",\" in keyword:\n",
    "                keywords.extend([k.lower().strip().replace(\" \" , \"_\") for k in keyword.split(\",\")])\n",
    "            else:\n",
    "                keywords.append(keyword)\n",
    "        return keywords\n",
    "    \n",
    "    def proceed(page):\n",
    "        if \"description\" in page \\\n",
    "                and \"keywords\" in page[\"description\"] \\\n",
    "                and len(page[\"description\"][\"keywords\"]) > 0:\n",
    "#             for keyword in get_keywords(page[\"description\"]):\n",
    "#                 valid = re.match('^[\\w-]+$', keyword) is not None\n",
    "#                 if not valid:\n",
    "#                     return False\n",
    "#                 if keyword not in ons_ft.wv.vocab:\n",
    "#                     return False\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def process_text(text, corpus, min_sent_length=4):\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            if len(tokens) >= min_sent_length:\n",
    "                doc = {\"text\": sentence, \"keywords\": keywords}        \n",
    "                corpus.append(doc)\n",
    "                \n",
    "    from progressbar import ProgressBar\n",
    "    pbar = ProgressBar()\n",
    "\n",
    "    corpus = []\n",
    "    for page in pbar(pages):\n",
    "        if proceed(page):\n",
    "            description = page[\"description\"]\n",
    "            keywords = get_keywords(description)\n",
    "            \n",
    "            text = description[\"title\"]\n",
    "#             process_text(text, corpus)\n",
    "                \n",
    "            if \"summary\" in description:\n",
    "                text = description[\"summary\"]\n",
    "                process_text(text, corpus)\n",
    "            if \"sections\" in page:\n",
    "                for section in page[\"sections\"]:\n",
    "                    if \"title\" in section:\n",
    "                        text = section[\"title\"]\n",
    "                        process_text(text, corpus)\n",
    "                    if \"markdown\" in section:\n",
    "                        text = markdown_to_text(section[\"markdown\"])\n",
    "                        process_text(text, corpus)\n",
    "                \n",
    "            \n",
    "    print \"Done\"\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "do_load=False\n",
    "\n",
    "if do_load:\n",
    "    with open(\"./raw_corpus.p\", \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "else:\n",
    "    corpus = generate_corpus(pages)\n",
    "    pickle.dump(np.array(corpus), open(\"./raw_corpus.p\", \"wb\"))\n",
    "print len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the training set\n",
    "The final step is to write the training data out in the correct format. We prepend a prefix to each label, so the classifier knows how to identify them aside from the raw text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = get_stopwords()\n",
    "stops.append(\".\")\n",
    "# stops = []\n",
    "\n",
    "def label_sentence(text, labels, label_prefix=\"__label__\"):\n",
    "    \n",
    "    try:\n",
    "        labels = filter(None, process_texts([labels], stops=stops))[0]\n",
    "        labels = [l for l in labels if l in ons_ft.wv.vocab]\n",
    "\n",
    "        if len(labels) > 0:\n",
    "            labels = set( [\"%s%s\" % (label_prefix, l) for l in labels] )\n",
    "            joined_labels = \" \".join(labels)\n",
    "\n",
    "            text = process_texts([text.split()], stops=stops)[0]\n",
    "            text = \" \".join(text)\n",
    "            if text.endswith(\".\") is False:\n",
    "                text += \".\"\n",
    "\n",
    "            return \"%s %s\" % (joined_labels, text)\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "print \"Processing %d items in corpus\" % len(corpus)\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "\n",
    "lines = []\n",
    "for d in pbar(corpus):\n",
    "    text = d[\"text\"].strip()\n",
    "    keywords = d[\"keywords\"]\n",
    "    line = label_sentence(text, keywords)\n",
    "    if line is not None:\n",
    "        lines.append(line.lower())\n",
    "    \n",
    "# Shuffle the list\n",
    "import random\n",
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(lines)\n",
    "print lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Data\n",
    "In order to test the accuracy of our model, we purposely keep part of the training data back as a validation dataset by splitting the corpus into two files, with extendions \".train\" and \".valid\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def write_corpus(full_corpus, prefix):\n",
    "    \"\"\"\n",
    "    Splits the corpus into training (.train) and validation (.valid) datasets\n",
    "    \"\"\"    \n",
    "    size_train = int(np.round(len(full_corpus) * (3./4.)))\n",
    "    train_corpus = full_corpus[:size_train]\n",
    "    valid_corpus = full_corpus[size_train:]\n",
    "    \n",
    "    print len(train_corpus)\n",
    "    print len(valid_corpus)\n",
    "    \n",
    "    extensions = [\"train\", \"valid\"]\n",
    "    corpa = [train_corpus, valid_corpus]\n",
    "    \n",
    "    for ext, cps in zip(extensions, corpa):\n",
    "        fname = \"%s_labelled.txt.%s\" % (prefix, ext)\n",
    "        with open(fname, \"w\") as f:\n",
    "            for s in cps:\n",
    "                if len(s) > 0:\n",
    "                    s = re.sub( '\\s+', ' ', s.encode(\"ascii\", \"ignore\") ).strip()\n",
    "                    f.write(\"%s\\n\" % s)\n",
    "\n",
    "# if os.path.isfile(corpus_fname) is False:\n",
    "write_corpus(lines, \"ons\")\n",
    "print \"Corpa written to file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import fastText\n",
    "print \"Training...\"\n",
    "model = fastText.train_supervised(input=\"ons_labelled.txt.train\", label=\"__label__\",\\\n",
    "                          dim=300, epoch=1000, wordNgrams=2, verbose=2, minCount=15,\\\n",
    "                          minCountLabel=5, lr=0.1, neg=10, thread=16, loss=\"ns\", t=1e-5)\n",
    "model.save_model(\"models/ons_supervised.bin\")\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "To test the model, we use the validation training set to try and predict the top 'k' labels. The output of the below test are the precision at k=1 (P@1) and the recall at k=1 (R@1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "k=1\n",
    "N, P, R = model.test(\"ons_labelled.txt.valid\", k)\n",
    "print \"Total number of samples=\", N\n",
    "print \"P@%d=\" % k, P\n",
    "print \"R@%d=\" % k, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the precision and recall at k=5 with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "k=5\n",
    "N, P, R = model.test(\"ons_labelled.txt.valid\", k)\n",
    "print \"Total number of samples=\", N\n",
    "print \"P@%d=\" % k, P\n",
    "print \"R@%d=\" % k, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "k=10\n",
    "N, P, R = model.test(\"ons_labelled.txt.valid\", k)\n",
    "print \"Total number of samples=\", N\n",
    "print \"P@%d=\" % k, P\n",
    "print \"R@%d=\" % k, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example output\n",
    "k = 10\n",
    "label_prefix = \"__label__\"\n",
    "labels, probs = model.predict(\"UKs shortfall to Germany\".lower(), k)\n",
    "for label,prob in zip(labels, probs):\n",
    "    print \"%s:%f\" % (label.replace(label_prefix, \"\"), prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduce memory usage of model and save\n",
    "# model.quantize()\n",
    "# model.save_model(\"models/ons_supervised_quantized.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "In this notebook, we have successfully trained a model which can predict ONS keywords from raw text, using published articles and bulletins for supervised training. Such a model can be used to recommend keywords based on raw, human written, text input or even classify search terms into keyword categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fastText\n",
    "\n",
    "model = fastText.load_model(\"models/ons_supervised.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = model.get_labels()\n",
    "norm = lambda vec: vec / np.linalg.norm(vec)\n",
    "\n",
    "om = model.get_output_matrix()\n",
    "word_vectors = np.zeros(om.shape)\n",
    "for i in range(len(om)):\n",
    "    word_vectors[i] = norm(om[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, u'__label__gdp')\n",
      "(0.5799029683224861, u'__label__unit')\n",
      "(0.53219564271997, u'__label__investment')\n",
      "(0.527457115069589, u'__label__sectors')\n",
      "(0.507968012949438, u'__label__brazil')\n",
      "(0.5021528824242844, u'__label__output')\n",
      "(0.5017086959446281, u'__label__growths')\n",
      "(0.4884507690957247, u'__label__annual')\n",
      "(0.4837299025589181, u'__label__international_comparisons')\n",
      "(0.4527872163462047, u'__label__recreation')\n",
      "(0.4477983338894497, u'__label__per')\n",
      "(0.4476581784113353, u'__label__sport')\n",
      "(0.4467532841136419, u'__label__hour')\n",
      "(0.4461745364758096, u'__label__mbs')\n",
      "(0.4304655989237989, u'__label__labour')\n",
      "(0.41896423913067865, u'__label__trade')\n",
      "(0.41696726450555627, u'__label__oecd')\n",
      "(0.3973072457027024, u'__label__precious_metals')\n",
      "(0.39129522577894055, u'__label__valuables')\n",
      "(0.39017144490078787, u'__label__gcf')\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors\n",
    "    :param vec1:\n",
    "    :param vec2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return cos_sim\n",
    "\n",
    "def cosine_sim2(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors\n",
    "    :param vec1:\n",
    "    :param vec2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cos_sim = [np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)) for v1,v2 in zip(vec1, vec2)]\n",
    "    return np.array(cos_sim)\n",
    "\n",
    "ix_gdp = np.where(np.array(words) == \"__label__gdp\")\n",
    "vec1 = norm(word_vectors[ix_gdp])\n",
    "d = np.zeros(len(word_vectors))\n",
    "for i,vec in enumerate(word_vectors):\n",
    "    d[i] = cosine_sim(vec1, vec)\n",
    "#     d[i] = euclidean(vec1, vec)\n",
    "    \n",
    "top_n = 20\n",
    "top_labels = sorted(zip(d, words), reverse=True)[:top_n]\n",
    "for top_label in top_labels:\n",
    "    print top_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing K-Means...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-d33376ba7496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mkMeansVar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mkMeansVar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meblow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-d33376ba7496>\u001b[0m in \u001b[0;36meblow\u001b[0;34m(data, n)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meblow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Performing K-Means...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mkMeansVar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkMeansVar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sullid/anaconda2/lib/python2.7/site-packages/sklearn/cluster/k_means_.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    897\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sullid/anaconda2/lib/python2.7/site-packages/sklearn/cluster/k_means_.pyc\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sullid/anaconda2/lib/python2.7/site-packages/sklearn/cluster/k_means_.pyc\u001b[0m in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;31m# init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n\u001b[0;32m--> 478\u001b[0;31m                               x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[1;32m    479\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initialization complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sullid/anaconda2/lib/python2.7/site-packages/sklearn/cluster/k_means_.pyc\u001b[0m in \u001b[0;36m_init_centroids\u001b[0;34m(X, k, init, random_state, x_squared_norms, init_size)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         centers = _k_init(X, k, random_state=random_state,\n\u001b[0;32m--> 684\u001b[0;31m                           x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[1;32m    685\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'random'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sullid/anaconda2/lib/python2.7/site-packages/sklearn/cluster/k_means_.pyc\u001b[0m in \u001b[0;36m_k_init\u001b[0;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mcenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_norm_squared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_squared_norms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         squared=True)\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mcurrent_pot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosest_dist_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# Pick the remaining n_clusters-1 points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def new_euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False): \n",
    "    return cosine_sim2(X, Y)\n",
    "\n",
    "# monkey patch (ensure cosine dist function is used)\n",
    "from sklearn.cluster import k_means_\n",
    "k_means_.euclidean_distances = new_euclidean_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "\n",
    "def eblow(data, n):\n",
    "    print \"Performing K-Means...\"\n",
    "    kMeansVar = [KMeans(n_clusters=k, random_state=0).fit(data) for k in range(1, n)]\n",
    "    print \"Done\"\n",
    "    centroids = [X.cluster_centers_ for X in kMeansVar]\n",
    "    print \"Computing Euclidean distances...\"\n",
    "    k_euclid = [cdist(data, cent) for cent in centroids]\n",
    "    dist = [np.min(ke, axis=1) for ke in k_euclid]\n",
    "    avgWithinSS = [sum(d)/data.shape[0] for d in dist]\n",
    "    print \"Done\"\n",
    "    wcss = [sum(d**2) for d in dist]\n",
    "    print \"Computing pairwise distances...\"\n",
    "    pair_dist = pdist(data)\n",
    "    print \"Computing percentage of variance explained...\"\n",
    "    tss = sum(pair_dist**2)/data.shape[0]\n",
    "    bss = tss - wcss\n",
    "    percentage_variance = (bss/tss)*100.0\n",
    "    \n",
    "    K = range(1,n)\n",
    "    seg_threshold = 0.95 #Set this to your desired target\n",
    "\n",
    "    #The angle between three points\n",
    "    def segments_gain(p1, v, p2):\n",
    "        vp1 = np.linalg.norm(p1 - v)\n",
    "        vp2 = np.linalg.norm(p2 - v)\n",
    "        p1p2 = np.linalg.norm(p1 - p2)\n",
    "        return np.arccos((vp1**2 + vp2**2 - p1p2**2) / (2 * vp1 * vp2)) / np.pi\n",
    "\n",
    "    #Normalize the data\n",
    "    criterion = np.array(avgWithinSS)\n",
    "    criterion = (criterion - criterion.min()) / (criterion.max() - criterion.min())\n",
    "\n",
    "    #Compute the angles\n",
    "    seg_gains = np.array([0, ] + [segments_gain(*\n",
    "            [np.array([K[j], criterion[j]]) for j in range(i-1, i+2)]\n",
    "        ) for i in range(len(K) - 2)] + [np.nan, ])\n",
    "\n",
    "    #Get the first index satisfying the threshold\n",
    "    kIdx = np.argmax(seg_gains > seg_threshold)\n",
    "    \n",
    "    print \"Plotting...\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(K[kIdx], percentage_variance[kIdx], marker='o', markersize=12, \n",
    "        markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "    \n",
    "    ax.scatter(K, percentage_variance, color='b')\n",
    "    ax.plot(K, percentage_variance, color='b')\n",
    "    \n",
    "    ax.set_xlabel(\"Number of Clusters\", fontsize=12)\n",
    "    ax.set_ylabel(\"Percentage of Variance Explained [%]\", fontsize=12)\n",
    "    \n",
    "    print \"Minimum number of clusters = \", K[kIdx]\n",
    "    \n",
    "    return kMeansVar\n",
    "    \n",
    "kMeansVar = eblow(word_vectors, 31)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "def matplotlib_to_plotly(cmap, pl_entries):\n",
    "    h = 1.0/(pl_entries-1)\n",
    "    pl_colorscale = []\n",
    "    \n",
    "    for k in range(pl_entries):\n",
    "        C = map(np.uint8, np.array(cmap(k*h)[:3])*255)\n",
    "        pl_colorscale.append([k*h, 'rgb'+str((C[0], C[1], C[2]))])\n",
    "        \n",
    "    return pl_colorscale\n",
    "\n",
    "def ncols(NUM_COLORS, cmap='gist_rainbow'):\n",
    "    import matplotlib.cm as mplcm\n",
    "    import matplotlib.colors as colors\n",
    "    \n",
    "    cm = plt.get_cmap(cmap)\n",
    "    cNorm  = colors.Normalize(vmin=0, vmax=NUM_COLORS-1)\n",
    "    scalarMap = mplcm.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "    return scalarMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798 798\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Sully0190/6.embed\" height=\"900px\" width=\"900px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "from sklearn import decomposition\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors\n",
    "    :param vec1:\n",
    "    :param vec2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return cos_sim\n",
    "\n",
    "kOpt = 10\n",
    "pca3 = decomposition.PCA(n_components=3)\n",
    "pca_fit3 = pca3.fit(word_vectors)\n",
    "X = pca_fit3.transform(word_vectors)\n",
    "words = model.get_labels()\n",
    "\n",
    "print len(words), len(word_vectors)\n",
    "\n",
    "NUM_COLORS = kOpt+1\n",
    "scalarMap = ncols(NUM_COLORS, cmap=\"jet\")\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=1,\n",
    "                          print_grid=False,\n",
    "                          specs=[[{'is_3d': True}]])\n",
    "\n",
    "def similarity(word1, word2, model):\n",
    "    w1 = model.get_word_vector(word1)\n",
    "    w2 = model.get_word_vector(word2)\n",
    "    return cosine_sim(w1, w2)\n",
    "\n",
    "scene = dict(\n",
    "    camera = dict(\n",
    "    up=dict(x=0, y=0, z=1),\n",
    "    center=dict(x=0, y=0, z=0),\n",
    "    eye=dict(x=2.5, y=0.1, z=0.1)\n",
    "    ),\n",
    "    xaxis=dict(title='pc1',\n",
    "        gridcolor='rgb(255, 255, 255)',\n",
    "        zerolinecolor='rgb(255, 255, 255)',\n",
    "        showbackground=True,\n",
    "        backgroundcolor='rgb(230, 230,230)',\n",
    "        showticklabels=False, ticks=''\n",
    "    ),\n",
    "    yaxis=dict(title='pc2',\n",
    "        gridcolor='rgb(255, 255, 255)',\n",
    "        zerolinecolor='rgb(255, 255, 255)',\n",
    "        showbackground=True,\n",
    "        backgroundcolor='rgb(230, 230,230)',\n",
    "        showticklabels=False, ticks=''\n",
    "    ),\n",
    "    zaxis=dict(title='pc3',\n",
    "        gridcolor='rgb(255, 255, 255)',\n",
    "        zerolinecolor='rgb(255, 255, 255)',\n",
    "        showbackground=True,\n",
    "        backgroundcolor='rgb(230, 230,230)',\n",
    "        showticklabels=False, ticks=''\n",
    "    )\n",
    ")\n",
    "\n",
    "name,est = ('k_means', KMeans(n_clusters=kOpt, random_state=0).fit(word_vectors))\n",
    "vector_idx = est.fit_predict(word_vectors)\n",
    "\n",
    "cols = [scalarMap.to_rgba(i) for i in range(NUM_COLORS)]\n",
    "\n",
    "# Add cluster centroids\n",
    "centroids = pca_fit3.transform(est.cluster_centers_)\n",
    "# centroidNames = [\"Centroid %d\" % (i+1) for i in range(kOpt)]\n",
    "\n",
    "for icluster in range(kOpt):\n",
    "    idx = np.where(vector_idx == icluster)[0]\n",
    "    labels = est.labels_[idx]\n",
    "    data = X[idx]\n",
    "    \n",
    "    vec = est.cluster_centers_[icluster]\n",
    "    name=\"Centroid %d\" % icluster\n",
    "\n",
    "    text = []\n",
    "    for ix in idx:\n",
    "        text.append(words[ix])\n",
    "    pcols = [cols[icluster]] * len(idx)\n",
    "\n",
    "    trace = go.Scatter3d(x=data[:, 0], y=data[:, 1], z=data[:, 2],\n",
    "                         name=name,\n",
    "                         text=text,\n",
    "                         showlegend=True,\n",
    "                         mode='markers',\n",
    "                         marker=dict(\n",
    "                                color=pcols[:],\n",
    "                                line=dict(color='black', width=1)\n",
    "        ))\n",
    "    fig.append_trace(trace, 1, 1)\n",
    "    \n",
    "    centroidsTrace = go.Scatter3d(x=centroids[icluster, 0], y=centroids[icluster, 1], z=centroids[icluster, 2],\n",
    "                         name=name,\n",
    "                         text=\"Cluster %d\" % (icluster+1),\n",
    "                         showlegend=False,\n",
    "                         mode='markers',\n",
    "                         marker=dict(\n",
    "                                color=\"black\",\n",
    "                                line=dict(color='black', width=1)\n",
    "        ))\n",
    "    fig.append_trace(centroidsTrace, 1, 1)\n",
    "\n",
    "fig['layout'].update(height=900, width=900,\n",
    "                     margin=dict(l=10,r=10))\n",
    "\n",
    "fig['layout']['scene1'].update(scene)\n",
    "fig['layout']['scene2'].update(scene)\n",
    "fig['layout']['scene3'].update(scene)\n",
    "fig['layout']['scene4'].update(scene)\n",
    "fig['layout']['scene5'].update(scene)\n",
    "\n",
    "# Use py.iplot() for IPython notebook\n",
    "py.iplot(fig, filename='3d ONS point clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(array([  1,   5,   6,  10,  11,  12,  13,  48,  58,  69,  79,  87, 107,\n",
      "       135, 149, 167, 178, 183, 201, 208, 210, 211, 219, 256, 261, 292,\n",
      "       303, 308, 319, 368, 369, 405, 409, 422, 423, 469, 514, 549, 592,\n",
      "       600, 601, 602, 603, 618, 628, 664, 665, 666, 669, 690, 732, 733,\n",
      "       736, 742, 747, 749, 769, 771, 776]),)\n",
      "59 59\n"
     ]
    }
   ],
   "source": [
    "ix = np.where(np.array(words) == \"__label__gdp\")\n",
    "\n",
    "gdp_cluster = vector_idx[ix][0]\n",
    "print gdp_cluster\n",
    "\n",
    "cluster_words_ix = np.where(vector_idx == gdp_cluster)\n",
    "print cluster_words_ix\n",
    "cluster_words = np.array(words)[cluster_words_ix]\n",
    "cluster_vecs = np.array(word_vectors)[cluster_words_ix]\n",
    "print len(cluster_words), len(cluster_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
